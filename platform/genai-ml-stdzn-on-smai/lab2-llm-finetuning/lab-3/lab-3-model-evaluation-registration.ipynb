{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lab 3: Model Evaluation and Registry with Governance\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, you'll complete the governance lifecycle by evaluating your fine-tuned model and registering it in the **SageMaker Model Registry**. This establishes a centralized catalog for model versioning, approval workflows, and deployment tracking.\n",
    "\n",
    "### What You'll Accomplish\n",
    "- Compare fine-tuned vs. base model performance\n",
    "- Create model cards with governance metadata\n",
    "- Register models in SageMaker Model Registry\n",
    "- Establish approval workflows for production deployment\n",
    "\n",
    "### Why Model Registry Matters for Governance\n",
    "- **Version Control**: Track all model versions with complete lineage\n",
    "- **Approval Workflows**: Require manual approval before production deployment\n",
    "- **Centralized Catalog**: Single source of truth for all models\n",
    "- **Deployment Tracking**: Know which model version is deployed where\n",
    "- **Model Cards**: Document model purpose, risks, and business context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Setup and Dependencies\n",
    "\n",
    "Install required libraries and restart the kernel to ensure all packages are properly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker==2.253.1 datasets==4.4.1 mlflow==3.5.1 tiktoken evaluate==0.4.0 rouge_score metrics --quiet \n",
    "# restart kernel\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "# Initialize SageMaker session\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = sess.boto_region_name\n",
    "account_id = role.split(':')[4]\n",
    "\n",
    "# Replace it with your account id and region. You can also find the bucket to use from the CloudFormation output: DataBucketName\n",
    "bucket = \"{replace-with-your-DataBucketName-from-cloudformation-output}\"\n",
    "region = sess.boto_region_name\n",
    "\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "\n",
    "print(f\"Amazon SageMaker role: {role}\")\n",
    "print(f\"Amazon S3 bucket: {bucket}\")\n",
    "print(f\"AWS Region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Connect to MLflow App \n",
    "\n",
    "Retrieve the MLflow App detail that stores all experiment metadata from Lab 3. This connection allows us to access model artifacts, parameters, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = sm_client.list_mlflow_apps(MaxResults=10)\n",
    "    mlflow_apps = response.get('Summaries', [])\n",
    "    \n",
    "    if mlflow_apps:\n",
    "        active_apps = [s for s in mlflow_apps if s['Status'] == 'Created']\n",
    "        \n",
    "        if active_apps:\n",
    "            mlflow_app_arn = active_apps[0]['Arn']\n",
    "            mlflow_app_name = active_apps[0]['Name']\n",
    "            print(f\"âœ“ Found active MLflow App:\")\n",
    "            print(f\"  Name: {mlflow_app_name}\")\n",
    "            print(f\"  ARN: {mlflow_app_arn}\")\n",
    "        else:\n",
    "            print(\"âš  No active MLflow Apps found.\")\n",
    "            mlflow_app_arn = None\n",
    "    else:\n",
    "        print(\"âš  No MLflow Apps found in this region.\")\n",
    "        mlflow_app_arn = None\n",
    "except Exception as e:\n",
    "    print(f\"Error checking for MLflow Apps: {e}\")\n",
    "    mlflow_app_arn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Load data from the MLFlow run\n",
    "\n",
    "We use the `%store` magic command to retrieve variables saved from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r experiment_name\n",
    "%store -r fine_tuned_model_endpoint_name\n",
    "%store -r base_model_endpoint_name\n",
    "\n",
    "print(experiment_name)\n",
    "print(fine_tuned_model_endpoint_name)\n",
    "print(base_model_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(mlflow_app_arn)\n",
    "\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_names=[experiment_name],\n",
    "    filter_string=\"tags.mlflow.runName LIKE '%fine-tuning%'\",\n",
    "    order_by=[\"start_time DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "run_id = runs.iloc[0].run_id\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step 1: Model Evaluation\n",
    "\n",
    "### Why Evaluate Models?\n",
    "\n",
    "Before registering a model, we need to **quantify its performance** to:\n",
    "- Validate that fine-tuning improved the model\n",
    "- Establish baseline metrics for future comparisons\n",
    "- Document performance for governance and compliance\n",
    "- Make data-driven decisions about model deployment\n",
    "\n",
    "### Evaluation Metrics for Summarization\n",
    "\n",
    "We'll use standard NLP metrics:\n",
    "- **BLEU**: Measures n-gram overlap between generated and reference text\n",
    "- **ROUGE-1**: Unigram overlap (individual word matches)\n",
    "- **ROUGE-2**: Bigram overlap (two-word phrase matches)\n",
    "- **ROUGE-L**: Longest common subsequence (captures sentence structure)\n",
    "\n",
    "Higher scores indicate better alignment with ground truth summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Load Evaluation Metrics\n",
    "\n",
    "Import custom metric functions that calculate BLEU and ROUGE scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import rouge1, rouge2, rougeL, bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Retrieve Evaluation Dataset\n",
    "\n",
    "We retrieve the evaluation dataset that was logged to MLflow during Lab 3. This demonstrates **data lineage** - we can trace exactly which data was used to evaluate this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = mlflow.get_run(run_id)\n",
    "dataset_inputs = run.inputs.dataset_inputs\n",
    "\n",
    "dataset_info = next(\n",
    "    (d.dataset for d in dataset_inputs if any(tag.value == \"evaluation\" for tag in d.tags)),\n",
    "    None\n",
    ")\n",
    "dataset_info\n",
    "\n",
    "if dataset_info:\n",
    "    source = mlflow.data.get_source(dataset_info)\n",
    "    jsonl_path = source.load()\n",
    "else:\n",
    "    print(\"No dataset with context 'evaluation' found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "evaluation_dataset = pd.read_json(jsonl_path, orient='records', lines=True)\n",
    "evaluation_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Define Endpoint Invocation Function\n",
    "\n",
    "This helper function invokes SageMaker endpoints to get predictions from both the base model and fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "def invoke_endpoint(endpoint_name, payload):\n",
    "    \"\"\"\n",
    "    Invoke a SageMaker endpoint with the given payload.\n",
    "    \n",
    "    Args:\n",
    "        endpoint_name (str): Name of the SageMaker endpoint\n",
    "        payload: The data to send to the endpoint (can be JSON, image bytes, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        The response from the endpoint\n",
    "    \"\"\"\n",
    "    # Create a SageMaker runtime client\n",
    "    runtime_client = boto3.client('sagemaker-runtime')\n",
    "    \n",
    "    try:\n",
    "        # Call the endpoint\n",
    "        response = runtime_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='application/json',  # Adjust based on your endpoint's requirements\n",
    "            Body=json.dumps(payload) if isinstance(payload, (dict, list)) else payload\n",
    "        )\n",
    "        \n",
    "        # Get the response body\n",
    "        response_body = response['Body'].read().decode('utf-8')\n",
    "        \n",
    "        # Parse the response if it's JSON\n",
    "        try:\n",
    "            return json.loads(response_body)\n",
    "        except json.JSONDecodeError:\n",
    "            return response_body\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking endpoint: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Create Prompt Template\n",
    "\n",
    "Define the instruction format used during fine-tuning. This ensures consistent prompt formatting when evaluating both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Generate Predictions from Both Models\n",
    "\n",
    "This cell performs the actual evaluation by:\n",
    "1. Sending each test example to both the base model and fine-tuned model\n",
    "2. Collecting predictions from both models\n",
    "3. Comparing them against ground truth responses\n",
    "\n",
    "**Note**: This may take several minutes as we're invoking endpoints 20 times for each model (40 total invocations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_dataset = Dataset.from_pandas(evaluation_dataset)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "(\n",
    "    inputs,\n",
    "    ground_truth_responses,\n",
    "    responses_before_finetuning,\n",
    "    responses_after_finetuning,\n",
    ") = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": template[\"prompt\"].format(\n",
    "            instruction=datapoint[\"instruction\"], context=datapoint[\"context\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "    pretrained_response =invoke_endpoint(base_model_endpoint_name, payload)\n",
    "    responses_before_finetuning.append(pretrained_response.get(\"generated_text\"))\n",
    "    finetuned_response = invoke_endpoint(fine_tuned_model_endpoint_name, payload)\n",
    "    responses_after_finetuning.append(finetuned_response.get(\"generated_text\"))\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, datapoint in enumerate(test_dataset.select(range(20))):\n",
    "        predict_and_print(datapoint)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Inputs\": inputs,\n",
    "            \"Ground Truth\": ground_truth_responses,\n",
    "            \"Response from non-finetuned model\": responses_before_finetuning,\n",
    "            \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "        }\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Prepare Data for Evaluation\n",
    "\n",
    "Create DataFrames with predictions and ground truth for both models. MLflow's evaluate function requires this specific format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = min(len(responses_before_finetuning), len(ground_truth_responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = pd.DataFrame({\n",
    "    \"predictions\": responses_before_finetuning[:min_len],\n",
    "    \"targets\": ground_truth_responses[:min_len]\n",
    "})\n",
    "\n",
    "df_after = pd.DataFrame({\n",
    "    \"predictions\": responses_after_finetuning[:min_len],\n",
    "    \"targets\": ground_truth_responses[:min_len]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Calculate and Compare Metrics\n",
    "\n",
    "Use MLflow's evaluate function to calculate metrics for both models. This:\n",
    "- Automatically logs metrics to MLflow for tracking\n",
    "- Creates separate runs for base and fine-tuned model evaluation\n",
    "- Enables side-by-side comparison in the MLflow UI\n",
    "\n",
    "**Expected Outcome**: The fine-tuned model should show higher scores across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.ERROR)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run(run_name=f\"base-model-eval-{timestamp}\"):\n",
    "    result_before = mlflow.evaluate(\n",
    "        data=df_before,\n",
    "        targets=\"targets\",\n",
    "        predictions=\"predictions\",\n",
    "        extra_metrics=[bleu, rouge1, rouge2, rougeL]\n",
    "    )\n",
    "\n",
    "with mlflow.start_run(run_name=f\"fine-tuned-model-eval-{timestamp}\"):\n",
    "    result_after = mlflow.evaluate(\n",
    "        data=df_after,\n",
    "        targets=\"targets\",\n",
    "        predictions=\"predictions\",\n",
    "        extra_metrics=[bleu, rouge1, rouge2, rougeL]\n",
    "\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Base Model ===\")\n",
    "print(f\"BLEU:    {result_before.metrics['bleu']:.4f}\")\n",
    "print(f\"ROUGE-1: {result_before.metrics['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {result_before.metrics['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {result_before.metrics['rougeL']:.4f}\")\n",
    "\n",
    "print(\"\\n=== Fine-tuned Model ===\")\n",
    "print(f\"BLEU:    {result_after.metrics['bleu']:.4f}\")\n",
    "print(f\"ROUGE-1: {result_after.metrics['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {result_after.metrics['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {result_after.metrics['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Evaluation Results\n",
    "\n",
    "âœ… **The fine-tuned model outperforms the base model** across all metrics!\n",
    "\n",
    "This quantitative evidence:\n",
    "- Validates that fine-tuning was successful\n",
    "- Provides metrics for governance documentation\n",
    "- Justifies model registration and potential deployment\n",
    "- Creates a baseline for future model versions\n",
    "\n",
    "These metrics are now logged in MLflow and can be viewed in the tracking UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Step 2: Model Registration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Understanding SageMaker Model Registry\n",
    "\n",
    "The SageMaker Model Registry is a **centralized repository** for managing ML models throughout their lifecycle. It provides:\n",
    "\n",
    "**Key Features:**\n",
    "- **Model Package Groups**: Logical grouping of related model versions\n",
    "- **Versioning**: Automatic version tracking for each registered model\n",
    "- **Approval Status**: Workflow states (Pending, Approved, Rejected)\n",
    "- **Model Cards**: Embedded documentation with governance metadata\n",
    "- **Lineage**: Links to training jobs, datasets, and experiments\n",
    "\n",
    "**Governance Benefits:**\n",
    "- **Audit Trail**: Complete history of model versions and approvals\n",
    "- **Access Control**: IAM-based permissions for model deployment\n",
    "- **Compliance**: Documentation required for regulatory requirements\n",
    "- **Deployment Tracking**: Know which version is deployed where\n",
    "\n",
    "### Model Registration Workflow\n",
    "1. Retrieve model artifacts from MLflow\n",
    "2. Register model with approval status\n",
    "3. Create model card with business context\n",
    "4. Set up lifecycle stages (Development â†’ Staging â†’ Production)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Retrieve the Run details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Get Container Image\n",
    "\n",
    "For model registration, we need the **inference container image** used by JumpStart. This ensures the model can be deployed with the correct runtime environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = run.data.params\n",
    "container_image = params['image_uri']\n",
    "print(container_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Retrieve model artifact path\n",
    "\n",
    "We need to extract the S3 location of the trained model from MLflow. This demonstrates **lineage tracking** - connecting MLflow experiments to Model Registry entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "model_data = mlflow.artifacts.load_dict(run.info.artifact_uri + \"/model_info.json\")\n",
    "data = json.dumps(model_data)\n",
    "model_path = json.loads(data)['model_artifact']\n",
    "print(\"=======\")\n",
    "print(f\"Model artifact location in Amazon S3: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Create Model Package Group\n",
    "\n",
    "A **Model Package Group** is a logical container for related model versions. Think of it as a \"model family\" where:\n",
    "- All versions of the same model type are grouped together\n",
    "- Each registration creates a new version (1, 2, 3, etc.)\n",
    "- You can compare versions and track evolution over time\n",
    "\n",
    "Example: `llama-summarization-models` might contain v1 (initial), v2 (improved), v3 (production)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model package group\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "model_package_group_name = f\"llama-summarization-models-{timestamp}\"\n",
    "\n",
    "try:\n",
    "    model_package_group_arn = sm_client.create_model_package_group(\n",
    "        ModelPackageGroupName=model_package_group_name,\n",
    "        ModelPackageGroupDescription=\"Fine-tuned Llama models for text summarization\"\n",
    "    )\n",
    "    print(f\"Created model package group: {model_package_group_name}\")\n",
    "except sm_client.exceptions.ResourceInUse:\n",
    "    print(f\"Model package group already exists: {model_package_group_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "**To view the Model Package Group you just created:**\n",
    "1. Navigate to **SageMaker AI Studio**\n",
    "2. In the left sidebar, scroll down and click **Models**\n",
    "3. You'll see your model package group listed\n",
    "   \n",
    "![MLFlow Experiment](../../images/model-package-group.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Register Model Package\n",
    "\n",
    "Now we create the model package registration request with:\n",
    "- **Model Package Group**: Logical grouping (e.g., \"llama-summarization-models\")\n",
    "- **Container Image**: Inference runtime environment\n",
    "- **Model Data URL**: S3 location of model artifacts\n",
    "- **Model Card**: Governance metadata\n",
    "- **Approval Status**: PendingManualApproval (requires explicit approval)\n",
    "\n",
    "This creates a **versioned model entry** in the registry with complete lineage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Upload Metrics and Register Model\n",
    "\n",
    "This cell performs the actual model registration:\n",
    "\n",
    "**Step 1: Upload Metrics to S3**\n",
    "- Package evaluation metrics in the required JSON format\n",
    "- Upload to S3 so they can be referenced by the model package\n",
    "\n",
    "**Step 2: Create Model Package**\n",
    "- Links the model artifacts (model.tar.gz) from Lab 3\n",
    "- Associates the container image for inference\n",
    "- Attaches evaluation metrics\n",
    "- Sets approval status to `PendingManualApproval`\n",
    "\n",
    "**Approval Status Options:**\n",
    "- `PendingManualApproval`: Requires explicit approval (recommended for production)\n",
    "- `Approved`: Automatically approved (use for development/testing)\n",
    "- `Rejected`: Explicitly rejected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Upload metrics first\n",
    "metrics_report = {\n",
    "    \"multiclass_classification_metrics\": {\n",
    "        \"bleu\": {\"value\": result_after.metrics['bleu'], \"standard_deviation\": 0.0},\n",
    "        \"rouge1\": {\"value\": result_after.metrics['rouge1'], \"standard_deviation\": 0.0},\n",
    "        \"rouge2\": {\"value\": result_after.metrics['rouge2'], \"standard_deviation\": 0.0},\n",
    "        \"rougeL\": {\"value\": result_after.metrics['rougeL'], \"standard_deviation\": 0.0}\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('evaluation.json', 'w') as f:\n",
    "    json.dump(metrics_report, f)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file('evaluation.json', bucket, 'model-metrics/evaluation.json')\n",
    "\n",
    "# Register model with metrics and model card\n",
    "sm_client = boto3.client('sagemaker')\n",
    "model_package = sm_client.create_model_package(\n",
    "    ModelPackageGroupName=model_package_group_name,\n",
    "    ModelPackageDescription=\"Fine-tuned Llama 3.2 for summarization\",\n",
    "    ModelApprovalStatus=\"PendingManualApproval\",\n",
    "    ModelMetrics={\n",
    "        \"ModelQuality\": {\n",
    "            \"Statistics\": {\n",
    "                \"ContentType\": \"application/json\",\n",
    "                \"S3Uri\": f\"s3://{bucket}/model-metrics/evaluation.json\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    InferenceSpecification={\n",
    "        \"Containers\": [{\n",
    "            \"Image\": container_image,\n",
    "            \"ModelDataUrl\": model_path\n",
    "        }],\n",
    "        \"SupportedContentTypes\": [\"application/json\"],\n",
    "        \"SupportedResponseMIMETypes\": [\"application/json\"]\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "**To view it:**\n",
    "1. Switch back to SageMaker AI Studio\n",
    "2. In the Model Registry, click on the name of the model package group you created earlier.\n",
    "3.  You will see the latest version: Version 1\n",
    "4.  Click on it to view its details as per the following screenshot:\n",
    "   \n",
    "![MLFlow Experiment](../../images/model-package.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Create Model Card\n",
    "\n",
    "A **Model Card** is a structured document that provides transparency about a machine learning model. It's essential for:\n",
    "- **Governance**: Documents model purpose, risks, and limitations\n",
    "- **Compliance**: Meets regulatory requirements (e.g., EU AI Act, GDPR)\n",
    "- **Transparency**: Helps stakeholders understand model behavior\n",
    "- **Risk Management**: Identifies potential issues and mitigation strategies\n",
    "\n",
    "### Model Card Sections\n",
    "\n",
    "1. **Model Overview**: Creator, artifacts, version information\n",
    "2. **Intended Uses**: Purpose, use cases, risk rating\n",
    "3. **Business Details**: Problem statement, stakeholders, business unit\n",
    "4. **Training Details**: Methodology, datasets, performance metrics\n",
    "5. **Additional Information**: Ethical considerations, caveats, custom metadata\n",
    "\n",
    "This model card will be embedded in the Model Registry entry, creating a **permanent governance record**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Create Model Card Content\n",
    "\n",
    "Build the model card with comprehensive governance information. Each section serves a specific purpose:\n",
    "\n",
    "**Model Overview**: Who created it, where artifacts are stored\n",
    "**Intended Uses**: What problem it solves, risk assessment\n",
    "**Business Details**: Business context and stakeholders\n",
    "**Training Details**: How the model was trained\n",
    "**Additional Information**: Ethical considerations, custom metadata\n",
    "\n",
    "This information becomes part of the permanent model record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import json\n",
    "\n",
    "model_card_content = {\n",
    "    \"model_overview\": {\n",
    "        \"model_creator\": \"Data Science Team\",\n",
    "        \"model_artifact\": [model_path]  # You need to define s3_bucket and s3_key\n",
    "    },\n",
    "    \"intended_uses\": {\n",
    "        \"purpose_of_model\": \"Text summarisation\",\n",
    "        \"intended_uses\": \"Answer to summarisation questions\",\n",
    "        \"factors_affecting_model_efficiency\": \"Question complexity, technical domain coverage, input length\",\n",
    "        \"risk_rating\": \"Low\",\n",
    "        \"explanations_for_risk_rating\": \"Model provides informational summaries without making critical decisions\"\n",
    "\n",
    "    },\n",
    "   \"business_details\": {\n",
    "        \"business_problem\": \"Improve efficiency of technical support and documentation access\",\n",
    "        \"business_stakeholders\": \"Technical support team, Documentation team, End users\",\n",
    "        \"line_of_business\": \"Customer Support & Knowledge Management\"\n",
    "    },\n",
    "    \"training_details\": {\n",
    "        \"objective_function\": {\n",
    "            \"function\": \"Instruction fine-tuning\",\n",
    "            \"notes\": \"Fine-tuned Llama 3.2 3B on summarization tasks using Dolly dataset\"\n",
    "        },\n",
    "        \"training_observations\": \"Model trained to generate concise, accurate summaries of technical content\"\n",
    "    },\n",
    "    \"additional_information\": {\n",
    "        \"ethical_considerations\": \"Ensure fair lending practices, avoid discriminatory outcomes\",\n",
    "        \"caveats_and_recommendations\": \"Regular monitoring for model drift, periodic retraining with updated data\",\n",
    "        \"custom_details\": {\n",
    "            \"UseCaseId\": \"002\",\n",
    "            \"UseCaseName\": \"Summarisation\",\n",
    "            \"UseCaseStage\": \"Development\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "# Save the model card\n",
    "with open('model_card.json', 'w') as f:\n",
    "    json.dump(model_card_content, f, indent=2)\n",
    "print(model_card_content)\n",
    "print(\"Model card has been created and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1228d3f5-243f-42d0-8b7a-b262e70d5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_arn = model_package['ModelPackageArn']\n",
    "model_package_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.update_model_package(\n",
    "    ModelPackageArn=model_package_arn,\n",
    "    CustomerMetadataProperties={\n",
    "        \"creator\": \"Data Science Team\",\n",
    "        \"use_case\": \"Text Summarization\",\n",
    "        \"business_problem\": \"Improve efficiency of technical support\",\n",
    "        \"risk_rating\": \"Low\",\n",
    "        \"model_type\": \"Fine-tuned Llama 3.2 3B\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Attach Model Card to the Model Package Group\n",
    "\n",
    "Update the model card in SageMaker to associate it with the registered model package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_arn = model_package['ModelPackageArn']\n",
    "\n",
    "response = sm_client.create_model_card(\n",
    "    ModelCardName=f\"model-card-{model_package_group_name}\".replace(\"_\", \"-\"),\n",
    "    Content=json.dumps(model_card_content),\n",
    "    ModelCardStatus=\"Draft\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card_arn = response['ModelCardArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link it to the model package via metadata (must be string)\n",
    "sm_client.update_model_package(\n",
    "    ModelPackageArn=model_package_arn,\n",
    "    CustomerMetadataProperties={\n",
    "        \"model_card\": model_card_arn\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## Step 3: Model Lifecycle Management\n",
    "\n",
    "### Understanding Model Lifecycle Stages\n",
    "\n",
    "SageMaker Model Registry supports **lifecycle stages** to track model progression through your ML workflow:\n",
    "\n",
    "**Typical Stages:**\n",
    "- **Development**: Model is being developed and tested\n",
    "- **Staging**: Model is ready for pre-production testing\n",
    "- **Production**: Model is approved for production deployment\n",
    "- **Archived**: Model is retired from active use\n",
    "\n",
    "**Stage Status:**\n",
    "- **PendingApproval**: Awaiting review\n",
    "- **Approved**: Cleared for use in this stage\n",
    "- **Rejected**: Not approved for this stage\n",
    "\n",
    "This creates a **formal approval workflow** ensuring only validated models reach production.\n",
    "\n",
    "### Setting Lifecycle Stage\n",
    "\n",
    "We'll mark this model as `Development/Approved` since it has passed evaluation but isn't ready for production yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Update Model Lifecycle\n",
    "\n",
    "This function updates the lifecycle stage of the registered model. You can modify the `Stage` and `StageStatus` values based on your organization's workflow.\n",
    "\n",
    "**Customization Options:**\n",
    "- Change `Stage` to: Development, Staging, Production, or Archived\n",
    "- Change `StageStatus` to: PendingApproval, Approved, or Rejected\n",
    "- Add `StageDescription` to document why the model is in this stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Model Lifecycle\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "print(\"Boto3 version:\", boto3. __version__)\n",
    "\n",
    "def update_model_lifecycle(model_package_info, model_package_update_input_dict):\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    try:\n",
    "\n",
    "         # Extract the ARN from the model_package_info\n",
    "        model_package_arn = model_package_info.get('ModelPackageArn')\n",
    "        \n",
    "        if not model_package_arn:\n",
    "            raise ValueError(\"ModelPackageArn not found in the provided information\")\n",
    "\n",
    "        # Ensure ModelPackageArn is in the input dictionary\n",
    "        model_package_update_input_dict['ModelPackageArn'] = model_package_arn\n",
    "        response = sagemaker_client.update_model_package(**model_package_update_input_dict)\n",
    "        \n",
    "        print(f\"Model lifecycle updated successfully for {model_package_arn}\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating model lifecycle: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Update Model Cycle Info\n",
    "\n",
    "model_package_info = {\n",
    "    'ModelPackageGroupName': model_package_group_arn,\n",
    "    'ModelPackageArn': model_package_arn,\n",
    "}\n",
    "\n",
    "# Update the staging values as needed for your projects\n",
    "# cfr https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-staging-construct-set-up.html\n",
    "\n",
    "model_package_update_input_dict = {\n",
    "    'ModelLifeCycle': {\n",
    "        'Stage': 'Development',\n",
    "        'StageDescription': 'Model trained and evaluated in development environment',\n",
    "        'StageStatus': 'Approved' # PendingApproval/Approved/Rejected\n",
    "    },\n",
    "}\n",
    "\n",
    "result = update_model_lifecycle(model_package_info, model_package_update_input_dict)\n",
    "\n",
    "if result:\n",
    "    print(\"Update successful\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(\"Update failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "## Step 4. Delete SageMaker AI real-time endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "Now that you have completed all your evaluations, you can delete the SageMaker real-time endpoints you created in previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "def delete_all_endpoints():\n",
    "    \"\"\"Delete all existing SageMaker endpoints\"\"\"\n",
    "    \n",
    "    # Create a SageMaker client\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    \n",
    "    try:\n",
    "        # List all endpoints\n",
    "        endpoints = sagemaker_client.list_endpoints()\n",
    "        \n",
    "        if not endpoints['Endpoints']:\n",
    "            print(\"No endpoints found to delete.\")\n",
    "            return\n",
    "        \n",
    "        # Delete each endpoint\n",
    "        for endpoint in endpoints['Endpoints']:\n",
    "            endpoint_name = endpoint['EndpointName']\n",
    "            try:\n",
    "                print(f\"Deleting endpoint: {endpoint_name}\")\n",
    "                sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "                print(f\"Successfully deleted endpoint: {endpoint_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting endpoint {endpoint_name}: {str(e)}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error listing endpoints: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_all_endpoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations! Lab 3 Complete\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "In this lab, you successfully:\n",
    "\n",
    "1. âœ… **Evaluated model performance** using BLEU and ROUGE metrics\n",
    "2. âœ… **Compared base vs. fine-tuned models** with quantitative evidence\n",
    "3. âœ… **Created comprehensive model cards** with governance metadata\n",
    "4. âœ… **Registered models** in SageMaker Model Registry with versioning\n",
    "5. âœ… **Established approval workflows** with lifecycle management\n",
    "6. âœ… **Linked evaluation metrics** to model registry entries\n",
    "\n",
    "### Complete Governance Workflow\n",
    "\n",
    "**Lab 3 + Lab 4 = End-to-End ML Governance**\n",
    "\n",
    "Data Preparation â†’ Fine-Tuning â†’ MLflow Tracking â†’ Model Evaluation â†’ Model Registry â†’ Approval Workflow\n",
    "\n",
    "### Key Governance Capabilities\n",
    "\n",
    "- **Lineage Tracking**: Trace models back to training data\n",
    "- **Auditability**: Complete audit trail for compliance\n",
    "- **Reproducibility**: Recreate any model version exactly\n",
    "- **Compliance**: Model cards meet regulatory requirements\n",
    "- **Version Control**: Compare and rollback models\n",
    "- **Approval Workflows**: Prevent unauthorized deployments\n",
    "\n",
    "### Viewing Your Work\n",
    "\n",
    "- **SageMaker Console**: Navigate to Model Registry to view versions and model cards\n",
    "- **MLflow UI**: Compare evaluation runs side-by-side\n",
    "- **Programmatic Access**: Use boto3 to list model packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "\n",
    "### Next Steps\n",
    "\n",
    "- Approve models for staging/production environments\n",
    "- Implement CI/CD pipelines for automated deployment\n",
    "- Set up model monitoring for drift detection\n",
    "- Create governance dashboards\n",
    "\n",
    "**Thank you for completing Lab 3!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
