{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Fine-tune Llama 3.2 3B with Experiment Tracking\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab, you'll learn how to implement **model governance at scale** by fine-tuning a foundation model while automatically tracking all experimentation metadata. This will help you maintain auditability, reproducibility, and lineage in their ML workflows.\n",
    "\n",
    "\n",
    "### Use Case: Text Summarization\n",
    "\n",
    "You need a model that can **generate concise summaries** of longer text. This is valuable for:\n",
    "- **Financial Services**: Summarizing earnings reports, regulatory filings\n",
    "- **Healthcare**: Condensing patient notes, research papers\n",
    "- **Legal**: Summarizing contracts, case documents\n",
    "- **Customer Service**: Creating brief summaries of support tickets\n",
    "\n",
    "### What You'll Build\n",
    "A specialized **text summarization model** fine-tuned on the Dolly dataset using Amazon SageMaker JumpStart and Llama 3.2 3B model.\n",
    "As part of your fine-tuning, you will track:\n",
    "- Training hyperparameters\n",
    "- Data and model artifacts \n",
    "- Complete lineage from base model to fine-tuned version\n",
    "\n",
    "### Why This Matters for Governance\n",
    "- **Auditability**: Every training run is logged with timestamps, parameters, and results\n",
    "- **Reproducibility**: All experiments can be recreated from tracked metadata\n",
    "- **Lineage**: Clear chain from source data ‚Üí training job ‚Üí model artifacts ‚Üí deployments\n",
    "- **Compliance**: Meet regulatory requirements for model documentation and traceability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Install Dependencies\n",
    "\n",
    "First, you'll install the required libraries and initialize our SageMaker session. This establishes the execution context for our training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; background-color: #fff3cd; border-left: 5px solid #ffc107; color: #856404;\">\n",
    "<strong>‚ö†Ô∏è Important:</strong> The cell below installs libraries and restarts the kernel. After the restart, continue with the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker==2.253.1 datasets==4.4.1 mlflow==3.5.1 fsspec==2023.9.2 --quiet\n",
    "# restart kernel\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from packaging import version\n",
    "\n",
    "datasets_version = datasets.__version__\n",
    "print(f\"datasets version: {datasets_version}\")\n",
    "\n",
    "if version.parse(datasets_version) < version.parse(\"4.4.1\"):\n",
    "    print(\"‚ö†Ô∏è Warning: datasets version is below 4.4.1. Please run the previous cell again\")\n",
    "else:\n",
    "    print(\"‚úì Version OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "# Initialize SageMaker session\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "# Extract account ID from the role ARN\n",
    "# Role format: arn:aws:iam::ACCOUNT_ID:role/...\n",
    "account_id = role.split(':')[4]\n",
    "\n",
    "# Use pre-configured workshop bucket instead of default bucket\n",
    "# This avoids VPC endpoint policy restrictions on bucket creation\n",
    "# You can also find the bucket to use from the CloudFormation output: DataBucketName\n",
    "\n",
    "bucket = \"llm-fine-tuning-data-891377069427-us-east-1\"\n",
    "\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "\n",
    "print(f\"Amazon SageMaker role: {role}\")\n",
    "print(f\"Account ID: {account_id}\")\n",
    "print(f\"Amazon S3 bucket: {bucket}\")\n",
    "print(f\"AWS Region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Deploy base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will deploy the base Llama 3.2 3B model so that you can later compare its performance with the fine-tuned model for your summarization use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-3-2-3b\", \"1.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; border: 1px solid #c3e6cb; border-radius: 4px; padding: 12px; margin: 10px 0;\">\n",
    "<b>‚úì Llama Model EULA Acceptance</b><br>\n",
    "To deploy Llama models using SageMaker JumpStart, you must accept Meta's End User License Agreement (EULA). In the notebook, set <code>accept_eula=true</code> in the estimator configuration. By doing so, you acknowledge that you have read and agree to the terms of the EULA, available at https://ai.meta.com/resources/models-and-libraries/llama-downloads/. Deployment will fail if this parameter is not set to true.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **‚è±Ô∏è Note:** The deployment job will take approximately 10 minutes to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "pretrained_model = JumpStartModel(model_id=model_id, model_version=model_version, instance_type=\"ml.g5.2xlarge\")\n",
    "# Please change the following line to have accept_eula = True\n",
    "pretrained_predictor = pretrained_model.deploy(accept_eula=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "base_model_endpoint_name = pretrained_predictor.endpoint_name\n",
    "\n",
    "console_url = f\"https://console.aws.amazon.com/sagemaker/home?region={region}#/endpoints/{base_model_endpoint_name}\"\n",
    "\n",
    "display(Markdown(f\"You can **[view the Real-time Endpoint in the SageMaker Console]({console_url})**\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_model_endpoint_name)\n",
    "%store base_model_endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize MLflow for Experiment Tracking\n",
    "\n",
    "### Understanding SageMaker Managed MLflow\n",
    "\n",
    "SageMaker Serverless MLflow provides a **fully managed experiment MLflow App** that eliminates the need to set up and maintain your own MLflow infrastructure. Key benefits:\n",
    "\n",
    "- **Centralized Tracking**: All team members log to the same MLflow App\n",
    "- **No Infrastructure Management**: AWS handles scaling, backups, and availability\n",
    "- **Integrated Security**: Uses IAM for authentication and authorization\n",
    "- **Persistent Storage**: Experiments are stored durably in AWS-managed storage\n",
    "\n",
    "### What Gets Tracked\n",
    "When you log experiments to MLflow, you capture:\n",
    "- **Parameters**: Hyperparameters, model IDs, instance types\n",
    "- **Metrics**: Training loss, validation accuracy, custom metrics\n",
    "- **Artifacts**: Model files, training datasets, configuration files\n",
    "- **Metadata**: Run names, timestamps, tags, notes\n",
    "\n",
    "This creates a **complete audit trail** for governance and compliance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this lab as part of an AWS workshop, an MLFlow App has already been created for you. You can use this App to track your fine-tuning experiments. Let's retrieve the MLFlow App URI, you will use it to track the experiments.\n",
    "\n",
    "If you are running this notebook in your own environment, you need to have an existing running MLFlow App to be able to complete it successfully. Refer to the [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to set up your experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = sm_client.list_mlflow_apps(MaxResults=10)\n",
    "    mlflow_apps = response.get('Summaries', [])\n",
    "    \n",
    "    if mlflow_apps:\n",
    "        active_apps = [s for s in mlflow_apps if s['Status'] == 'Created']\n",
    "        \n",
    "        if active_apps:\n",
    "            mlflow_app_arn = active_apps[0]['Arn']\n",
    "            mlflow_app_name = active_apps[0]['Name']\n",
    "            print(f\"‚úì Found active MLflow App:\")\n",
    "            print(f\"  Name: {mlflow_app_name}\")\n",
    "            print(f\"  ARN: {mlflow_app_arn}\")\n",
    "        else:\n",
    "            print(\"‚ö† No active MLflow Apps found.\")\n",
    "            mlflow_app_arn = None\n",
    "    else:\n",
    "        print(\"‚ö† No MLflow Apps found in this region.\")\n",
    "        mlflow_app_arn = None\n",
    "except Exception as e:\n",
    "    print(f\"Error checking for MLflow Apps: {e}\")\n",
    "    mlflow_app_arn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Connect to the managed MLflow App\n",
    "mlflow.set_tracking_uri(mlflow_app_arn)\n",
    "\n",
    "# Create or use existing experiment\n",
    "# Experiments group related runs together (e.g., all summarization model iterations)\n",
    "experiment_name = f\"summarization-experiment-{timestamp}\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"‚úì MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"‚úì Experiment: {experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä View Your Experiment in MLflow\n",
    "\n",
    "**To access the MLflow UI:**\n",
    "\n",
    "1. In the left sidebar of SageMaker Studio, click the **MLflow** icon\n",
    "2. Click on your MlFlow App name\n",
    "3. Using the Menu on the right hand side, open MLFlow. (See screenshot below)\n",
    "4. Navigate to your experiment (See screenshot below. The exact experiment name will differ depending on the timestamp) \n",
    "![MLFlow Experiment](../../images/mlflow-console.png)\n",
    "![MLFlow Experiment](../../images/mlflow-experiment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next initiate a Run in your experiment to track the fine-tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new MLflow run to track this fine-tuning job\n",
    "# A \"run\" represents a single training execution with specific parameters\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "run_name=f\"llama-3.2-fine-tuning-summarization-{timestamp}\"\n",
    "\n",
    "mlflow_run = mlflow.start_run(run_name=run_name)\n",
    "print(f\"‚úì Started MLflow Run ID: {mlflow_run.info.run_id}\")\n",
    "print(f\"  This run will track all parameters, metrics, and artifacts from this training job.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare the Fine-Tuning Dataset\n",
    "\n",
    "### The Dolly Dataset\n",
    "In this step, you will prepare the dataset your will use to fine-tune your model.\n",
    "\n",
    "The Databricks Dolly dataset contains ~15,000 instruction-following examples across multiple categories:\n",
    "- Summarization\n",
    "- Question answering\n",
    "- Information extraction\n",
    "- Creative writing\n",
    "- Classification\n",
    "\n",
    "You'll filter for **summarization examples only** to create a domain-specific model.\n",
    "\n",
    "### Data Format for Instruction Tuning\n",
    "\n",
    "The data follows an instruction-tuning format:\n",
    "```json\n",
    "{\n",
    "  \"instruction\": \"Summarize the following text\",\n",
    "  \"context\": \"[Long text to summarize]\",\n",
    "  \"response\": \"[Expected summary]\"\n",
    "}\n",
    "```\n",
    "\n",
    "This teaches the model to follow instructions and generate appropriate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "summarization_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == \"summarization\")\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")\n",
    "\n",
    "# Split dataset: 70% for training the model, 30% held out to evaluate performance on unseen data\n",
    "train_and_test_dataset = summarization_dataset.train_test_split(test_size=0.3)\n",
    "\n",
    "train_and_test_dataset[\"train\"].to_json(\"train.jsonl\")\n",
    "train_and_test_dataset[\"test\"].to_json(\"test.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample training example:\")\n",
    "train_and_test_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Prompt Template\n",
    "\n",
    "### Why Prompt Templates Matter\n",
    "\n",
    "A **prompt template** defines how we structure inputs to the model. This is critical because:\n",
    "- The model was pre-trained with specific formatting conventions\n",
    "- Consistent formatting improves model performance\n",
    "- The same template must be used for training AND inference\n",
    "\n",
    "Your template follows the **instruction-input-response** pattern commonly used for instruction-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Upload Datasets to S3\n",
    "\n",
    "When fine-tuning a model using Amazon SageMaker JumpStart, it expects your dataset to be stored in Amazon S3. Below, we'll upload our prepared datasets and template to the Amazon S3 bucket path you have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/opt/ml/metadata/resource-metadata.json', 'r') as f:\n",
    "    profile_name = json.load(f)['UserProfileName']\n",
    "\n",
    "profile_name = profile_name[0].upper() + profile_name[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "# Use the workshop bucket defined in Step 1\n",
    "output_bucket = bucket\n",
    "data_location = f\"s3://{output_bucket}/{profile_name}/dolly_dataset\"\n",
    "\n",
    "train_path=\"train.jsonl\"\n",
    "template_path=\"template.json\"\n",
    "evaluation_path=\"test.jsonl\"\n",
    "\n",
    "training_input_path = f'{data_location}/{train_path}'\n",
    "eval_input_path = f'{data_location}/{evaluation_path}'\n",
    "\n",
    "S3Uploader.upload(train_path, data_location)\n",
    "S3Uploader.upload(template_path, data_location)\n",
    "S3Uploader.upload(evaluation_path, data_location)\n",
    "print(f\"Training data: {data_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Failed to determine whether UCVolumeDatasetSource')\n",
    "\n",
    "df_train = pd.read_json(train_path, orient=\"records\", lines=True)\n",
    "training_data = mlflow.data.from_pandas(df_train, source=training_input_path)\n",
    "mlflow.log_input(training_data, context=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluate = pd.read_json(evaluation_path, orient=\"records\", lines=True)\n",
    "df_evaluate.size\n",
    "evaluation_data = mlflow.data.from_pandas(df_evaluate, source=eval_input_path)\n",
    "mlflow.log_input(evaluation_data, context=\"evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Configure and Launch Fine-Tuning Job\n",
    "\n",
    "### SageMaker JumpStart Benefits\n",
    "\n",
    "JumpStart provides **pre-configured training scripts** for popular foundation models, eliminating the need to write custom training code. Benefits include optimized training configurations, support for distributed training, and integration with other SageMaker features. Before you start your fine-tuning job, you can also modify the hyperparameters for the model training.\n",
    "\n",
    "## Key Hyperparameters\n",
    "\n",
    "- **epochs**: Number of complete passes through the training data \n",
    "- **learning_rate**: Step size for model updates \n",
    "- **instruction_tuned**: Use instruction-following format \n",
    "- **per_device_train_batch_size**: Examples processed per GPU \n",
    "- **max_input_length**: Maximum tokens in input\n",
    "\n",
    "All hyperparameters are logged to MLflow for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; border: 1px solid #c3e6cb; border-radius: 4px; padding: 12px; margin: 10px 0;\">\n",
    "<b>‚úì Llama Model EULA Acceptance</b><br>\n",
    "To deploy Llama models using SageMaker JumpStart, you must accept Meta's End User License Agreement (EULA). In the notebook, set <code>accept_eula=true</code> in the estimator configuration. By doing so, you acknowledge that you have read and agree to the terms of the EULA, available at https://ai.meta.com/resources/models-and-libraries/llama-downloads/. Deployment will fail if this parameter is not set to true.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model ID for Llama 3.2 3B \n",
    "model_id = \"meta-textgeneration-llama-3-2-3b\"\n",
    "model_version = \"*\"  # Use latest version\n",
    "\n",
    "# Configure hyperparameters\n",
    "hyperparameters = {\n",
    "    \"epoch\": \"2\",\n",
    "    \"instruction_tuned\": \"True\",\n",
    "    \"max_input_length\": \"1024\",\n",
    "}\n",
    "\n",
    "# Log all hyperparameters to MLflow\n",
    "mlflow.log_param(\"base_model_id\", model_id)\n",
    "mlflow.log_param(\"model_version\", model_version)\n",
    "for key, value in hyperparameters.items():\n",
    "    mlflow.log_param(key, value)\n",
    "\n",
    "print(\"‚úì Hyperparameters configured and logged to MLflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JumpStart estimator for fine-tuning\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "run_name=f\"llama-3.2-fine-tuning-summarization-{timestamp}\"\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    role=role,\n",
    "    instance_type=instance_type,  # GPU instance for training\n",
    "    instance_count=1,\n",
    "    hyperparameters=hyperparameters,\n",
    "    disable_output_compression= False,\n",
    "    output_path=f\"s3://{bucket}/{profile_name}/model-output/\",  #Explicitly use workshop bucket for model artifacts\n",
    "    environment={\n",
    "        \"accept_eula\": \"true\", # CHANGED: Set to true to accept Meta's Llama EULA\n",
    "        \"MLFLOW_TRACKING_URI\": mlflow_app_arn,\n",
    "        \"MLFLOW_EXP\": experiment_name,\n",
    "        \"MLFLOW_RUN_NAME\": run_name\n",
    "    }\n",
    ")\n",
    "\n",
    "# Log training configuration\n",
    "mlflow.log_param(\"instance_type\", instance_type)\n",
    "mlflow.log_param(\"instance_count\", 1)\n",
    "mlflow.log_param(\"output_path\", f\"s3://{bucket}/{profile_name}/model-output/\")  #Log the output path\n",
    "mlflow.log_param(\"image_uri\", estimator.image_uri)\n",
    "\n",
    "print(\"‚úì Training estimator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "console_url = f\"https://console.aws.amazon.com/sagemaker/home?region={region}#/training\"\n",
    "\n",
    "display(Markdown(f\"You are now ready to start the training job and fine-tune the model. You can review the metadata and progress of the training job in the AWS console **[üîó View Training Jobs in SageMaker Console]({console_url})**\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **‚è±Ô∏è Note:** The training job will take approximately 15-18 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the fine-tuning job\n",
    "print(\"üöÄ Starting fine-tuning job...\")\n",
    "print(\"   This will take approximately 15 minutes.\")\n",
    "print(\"   You can monitor progress in the SageMaker console.\\n\")\n",
    "\n",
    "estimator.fit({\"training\": training_input_path}, logs=True)\n",
    "\n",
    "print(\"\\n‚úì Fine-tuning job completed!\")\n",
    "\n",
    "# Log training job details to MLflow\n",
    "mlflow.log_param(\"training_job_name\", estimator.latest_training_job.name)\n",
    "mlflow.log_param(\"model_artifact_s3\", estimator.model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Track the training progress\n",
    "While waiting, you can track the training progress above and also review the information you have logged in MLFLow:\n",
    "1. Navigate to the MLFlow console\n",
    "2. Find the summarization - experiment you created earlier\n",
    "3. Click on its name to view the experiment details\n",
    "4. Locate the Run and click on its name to view its details\n",
    "\n",
    "![MLFlow Experiment](../../images/run.png)\n",
    "![MLFlow Experiment](../../images/run_details.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_dict(\n",
    "    {\n",
    "        \"model_artifact\": estimator.model_data,\n",
    "    },\n",
    "    \"model_info.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "output_path = estimator.output_path\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "\n",
    "s3_url = f\"{output_path}{training_job_name}/output/model.tar.gz\"\n",
    "s3_path = s3_url.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "console_url = f\"https://s3.console.aws.amazon.com/s3/object/{s3_path[0]}?prefix={s3_path[1]}\"\n",
    "\n",
    "display(Markdown(f\"**Training Output:** The fine-tuned model artifacts are saved at **[{s3_url}]({console_url})**\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Deploy the Fine-Tuned Model\n",
    "\n",
    "Now we'll deploy the fine-tuned model to a SageMaker endpoint for real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **‚è±Ô∏è Note:** The deployment job will take approximately 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_predictor = estimator.deploy(instance_type=\"ml.g5.2xlarge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model_endpoint_name = finetuned_predictor.endpoint_name\n",
    "print(fine_tuned_model_endpoint_name)\n",
    "%store fine_tuned_model_endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log deployment details to MLflow\n",
    "mlflow.log_param(\"endpoint_name\", finetuned_predictor.endpoint_name)\n",
    "mlflow.log_param(\"endpoint_instance_type\", \"ml.g5.2xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Compare the Based and Fine-Tuned Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now do some initial testing to compare the outputs of the base and fine-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_response(model_id, payload, response):\n",
    "    print(f\"Model: {model_id}\")\n",
    "    print(f\"Prompt: {payload[\"inputs\"]}\")\n",
    "    print(f\"Response: {response.get('generated_text')}\")\n",
    "    print(\"\\n==================================\\n\")\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": \"\"\"### Instruction: What is Amazon SageMaker in one sentence?### Response:\\n\"\"\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False,\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    response = finetuned_predictor.predict(\n",
    "        payload, custom_attributes=\"accept_eula=true\"  # Please change this to \"accept_eula=true\"\n",
    "    )\n",
    "    print_response(model_id, payload, response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "test_dataset = train_and_test_dataset[\"test\"]\n",
    "\n",
    "(\n",
    "    inputs,\n",
    "    ground_truth_responses,\n",
    "    responses_before_finetuning,\n",
    "    responses_after_finetuning,\n",
    ") = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": template[\"prompt\"].format(\n",
    "            instruction=datapoint[\"instruction\"], context=datapoint[\"context\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "    # Please change the following line to \"accept_eula=true\"\n",
    "    pretrained_response = pretrained_predictor.predict(\n",
    "        payload, custom_attributes=\"accept_eula=true\"\n",
    "    )\n",
    "    responses_before_finetuning.append(pretrained_response.get(\"generated_text\"))\n",
    "    # Fine Tuned Llama 3 models doesn't required to set \"accept_eula=true\"\n",
    "    finetuned_response = finetuned_predictor.predict(payload)\n",
    "    responses_after_finetuning.append(finetuned_response.get(\"generated_text\"))\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "        predict_and_print(datapoint)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Inputs\": inputs,\n",
    "            \"Ground Truth\": ground_truth_responses,\n",
    "            \"Response from non-finetuned model\": responses_before_finetuning,\n",
    "            \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "        }\n",
    "    )\n",
    "    display(HTML(df.to_html()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store experiment_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Review Governance Artifacts in MLflow\n",
    "\n",
    "### What We've Tracked\n",
    "\n",
    "Throughout this lab, we've automatically logged:\n",
    "\n",
    "1. **Data Lineage**\n",
    "   - Source dataset location\n",
    "   - Number of training/test examples\n",
    "   - Data preprocessing steps\n",
    "\n",
    "2. **Model Lineage**\n",
    "   - Base model ID and version\n",
    "   - All hyperparameters\n",
    "   - Training job name\n",
    "   - Model artifact location\n",
    "\n",
    "3. **Deployment Lineage**\n",
    "   - Endpoint container image\n",
    "   - Instance type\n",
    "\n",
    "### Accessing Your Experiments\n",
    "\n",
    "You can view all tracked experiments in:\n",
    "1. **SageMaker Studio**: Navigate to MLflow App\n",
    "2. **MLflow UI**: Access through the MLflow App URL\n",
    "3. **Programmatically**: Query using MLflow APIs\n",
    "\n",
    "Since the training is now complete, let's mark the Run as completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # End the MLflow run\n",
    "mlflow.end_run()\n",
    "run_id=mlflow_run.info.run_id\n",
    "\n",
    "print(\"‚úì MLflow run completed\")\n",
    "print(f\"\\nRun Summary:\")\n",
    "print(f\"  Experiment: {experiment_name}\")\n",
    "print(f\"  Run ID: {mlflow_run.info.run_id}\")\n",
    "print(f\"  Run Name: llama-3.2-fine-tuning-summarization\")\n",
    "print(f\"\\nAll parameters, metrics, and artifacts have been logged for governance and auditability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "In this lab, you learned how to:\n",
    "\n",
    "1. ‚úÖ **Fine-tune a foundation model** for a specific use case (text summarization)\n",
    "2. ‚úÖ **Track all experimentation** using SageMaker Managed MLflow\n",
    "3. ‚úÖ **Establish complete lineage** from data ‚Üí training ‚Üí deployment\n",
    "4. ‚úÖ **Create audit trails** for governance and compliance\n",
    "5. ‚úÖ **Enable reproducibility** by logging all parameters and artifacts\n",
    "\n",
    "### Governance Benefits Demonstrated\n",
    "\n",
    "- **Auditability**: Every training run is logged with complete metadata\n",
    "- **Reproducibility**: Any experiment can be recreated from tracked parameters\n",
    "- **Lineage**: Clear chain from source data to deployed model\n",
    "- **Compliance**: Meet regulatory requirements for model documentation\n",
    "- **Collaboration**: Team members can view and compare all experiments\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- In the next lab, you will evaluate your fine-tuned model, review the metrics, and register it to the SageMaker Model Registry."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
